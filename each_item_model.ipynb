{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 아이템을 대상으로 train 하는게 아닌, 각 카테고리별로 train 시키고 그거를 합치는 방식으로 진행해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 로드 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 라이브러리 로드 및 메모리 감소 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[13:31:47] C:\\Jenkins\\workspace\\xgboost-win64_release_0.90\\dmlc-core\\src\\io\\local_filesys.cc:127: LocalFileSystem.ListDirectory demo/data error: No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2811ed2153e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# read in data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'demo/data/agaricus.txt.train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'demo/data/agaricus.txt.test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# specify parameters via map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kjb\\data_analysis\\10_estimate_sales_of_walmart_retail_goods\\venv\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    395\u001b[0m             _check_call(_LIB.XGDMatrixCreateFromFile(c_str(data),\n\u001b[0;32m    396\u001b[0m                                                      \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                                      ctypes.byref(handle)))\n\u001b[0m\u001b[0;32m    398\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kjb\\data_analysis\\10_estimate_sales_of_walmart_retail_goods\\venv\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \"\"\"\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [13:31:47] C:\\Jenkins\\workspace\\xgboost-win64_release_0.90\\dmlc-core\\src\\io\\local_filesys.cc:127: LocalFileSystem.ListDirectory demo/data error: No such process"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('demo/data/agaricus.txt.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def write_record(features, params):\n",
    "    record = open(\"record model and features.txt\", 'a')\n",
    "    record.write(\"\\n\")\n",
    "    record.write(str(datetime.datetime.now())+\"\\n\")\n",
    "\n",
    "    check = 0\n",
    "    for _ in features:\n",
    "        check += 1\n",
    "        if check % 5 == 0:\n",
    "            record.write(\"\\n\")\n",
    "        record.write(_+\"  \")\n",
    "    record.write(\"\\n\")\n",
    "    for i  in params.items():\n",
    "        record.write(str(i) + \"\\n\")\n",
    "\n",
    "    record.write('--------------------------------\\n')\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 데이터 불러오기 및 pd.melt를 활용해 데이터 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train = pd.melt(train, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='target')\n",
    "\n",
    "test = pd.read_csv('inputs/sample_submission.csv')\n",
    "test2 = test[30490:]\n",
    "\n",
    "test = test[:30490]\n",
    "test = pd.melt(test, id_vars=['id'], var_name='d', value_name='target')\n",
    "for i in range(1, 29):\n",
    "    test = test.replace({f'F{i}': f'd_{1913+i}'})\n",
    "\n",
    "test[['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id', 'tmp']] = pd.DataFrame(test['id'].str.split('_').tolist())\n",
    "del test['tmp']\n",
    "test['store_id'] = test['state_id'] + '_' + test['store_id']\n",
    "test['dept_id'] = test['cat_id'] + '_' + test['dept_id']\n",
    "test['item_id'] = test['dept_id'] + '_' + test['item_id']\n",
    "\n",
    "test = test[train.columns]\n",
    "\n",
    "calendar = pd.read_csv('inputs/calendar.csv')\n",
    "\n",
    "sell_prices = pd.read_csv('inputs/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아웃라이어를 제거해서 boxplot 더 잘보이게끔\n",
    "train200 = train[train['target'] < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 419)\n",
    "pd.DataFrame(train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "display(\n",
    "    train[train['target'] == 763],\n",
    "    train[train['id'] == 'FOODS_3_090_CA_3_validation'],\n",
    "    sns.distplot(train[train['id'] == 'FOODS_3_090_CA_3_validation']['target'])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "display(\n",
    "    train[train['target'] == 370],\n",
    "    train[train['id'] == 'FOODS_3_318_CA_3_validation'],\n",
    "    sns.distplot(train[train['id'] == 'FOODS_3_318_CA_3_validation']['target'])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train[train['target'] == 763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby('id')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('item_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 dept_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.groupby('dept_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(train['dept_id'], train200['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('cat_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby('store_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 state_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('state_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('d')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분 date 데이터고, event 및 snap(정부보조금 적용되는 날)에 집중해보자.\n",
    "- event_name_2가 너무 적다. 우선을 빼고 모델 만들어 볼 것임.\n",
    "- 이벤트 유무(is_event) 컬럼 만들 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = calendar.drop(['event_name_2', 'event_type_2'], axis=1)\n",
    "calendar['is_event'] = calendar['event_name_1'].notna().astype('int8')\n",
    "del calendar['wday']\n",
    "del calendar['wm_yr_wk'] # 모든 주차에 인덱스 붙여놓음. 282개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar['day'] = calendar['date'].astype('datetime64').dt.day\n",
    "calendar['week'] = calendar['date'].astype('datetime64').dt.week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(calendar, how='left')\n",
    "test = test.merge(calendar, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day'] = train['date'].astype('datetime64').dt.day\n",
    "test['day'] = test['date'].astype('datetime64').dt.day\n",
    "\n",
    "train['week'] = train['date'].astype('datetime64').dt.week\n",
    "test['week'] = test['date'].astype('datetime64').dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sell_prices['wm_yr_wk'].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.groupby('wm_yr_wk')['sell_price'].mean().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(range(len(sell_prices.groupby('wm_yr_wk')['sell_price'].mean())), list(sell_prices.groupby('wm_yr_wk')['sell_price'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18,
     46
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def write_record(features, params):\n",
    "    record = open(\"record model and features.txt\", 'a')\n",
    "    record.write(\"\\n\")\n",
    "    record.write(str(datetime.datetime.now())+\"\\n\")\n",
    "\n",
    "    check = 0\n",
    "    for _ in features:\n",
    "        check += 1\n",
    "        if check % 5 == 0:\n",
    "            record.write(\"\\n\")\n",
    "        record.write(_+\"  \")\n",
    "    record.write(\"\\n\")\n",
    "    for i  in params.items():\n",
    "        record.write(str(i) + \"\\n\")\n",
    "\n",
    "    record.write('--------------------------------\\n')\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train = pd.melt(train, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='target')\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('inputs/sample_submission.csv')\n",
    "test = test[:30490]\n",
    "test = pd.melt(test, id_vars=['id'], var_name='d', value_name='target')\n",
    "for i in range(1, 29):\n",
    "    test = test.replace({f'F{i}': f'd_{1913+i}'})\n",
    "\n",
    "test[['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id', 'tmp']] = pd.DataFrame(test['id'].str.split('_').tolist())\n",
    "del test['tmp']\n",
    "test['store_id'] = test['state_id'] + '_' + test['store_id']\n",
    "test['dept_id'] = test['cat_id'] + '_' + test['dept_id']\n",
    "test['item_id'] = test['dept_id'] + '_' + test['item_id']\n",
    "\n",
    "test = test[train.columns]\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 calendar (date 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('inputs/calendar.csv')\n",
    "\n",
    "calendar = calendar.drop(['event_name_2', 'event_type_2'], axis=1)\n",
    "calendar['is_event'] = calendar['event_name_1'].notna().astype('int8')\n",
    "del calendar['wday']  # weekday랑 똑같은 컬럼.\n",
    "calendar['day'] = calendar['date'].astype('datetime64').dt.day\n",
    "calendar['week'] = calendar['date'].astype('datetime64').dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(calendar, how='left')\n",
    "test = test.merge(calendar, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = pd.read_csv('inputs/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "test = test.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 라벨인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_df = pd.concat([train, test])\n",
    "\n",
    "# del all_df['date']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in all_df.columns[all_df.dtypes == 'object']:\n",
    "    if i == 'id' or i == 'date':\n",
    "        continue\n",
    "    all_df[i] = le.fit_transform(list(all_df[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 lag 데이터 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['lag_t28'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28))\n",
    "all_df['lag_t29'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(29))\n",
    "all_df['lag_t30'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 이동평균 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['rolling_mean_t7'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "all_df['rolling_std_t7'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "\n",
    "all_df['rolling_mean_t30'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "all_df['rolling_std_t30'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "\n",
    "all_df['rolling_mean_t90'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "all_df['rolling_mean_t180'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28).rolling(180).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 price 통계량 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['lag_price_t1'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "\n",
    "\n",
    "all_df['price_change_t1'] = (all_df['lag_price_t1'] - all_df['sell_price']) / (all_df['lag_price_t1'])\n",
    "all_df['rolling_price_max_t365'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "all_df['price_change_t365'] = (all_df['rolling_price_max_t365'] - all_df['sell_price']) / (all_df['rolling_price_max_t365'])\n",
    "all_df['rolling_price_std_t7'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "all_df['rolling_price_std_t30'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "all_df.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "# rf.fit(train2, np.log(train['target'] + 1))\n",
    "# result = rf.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMRegressor\n",
    "# # lgb = LGBMRegressor(num_leaves=2000, colsample_bytree=0.6, subsample=0.6, n_estimators=600, learning_rate=0.025, n_jobs=-1, device='gpu', max_bin = 63)\n",
    "# lgb = LGBMRegressor(num_leaves=20, colsample_bytree=0.6, subsample=0.6, n_estimators=60, learning_rate=0.02, n_jobs=-1, device='cpu')\n",
    "\n",
    "# lgb.fit(train, target)\n",
    "# result = lgb.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 train set /  test set 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'weekday', 'event_name_1', 'event_type_1',  \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n",
    "            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30']\n",
    "\n",
    "# 나중에 합칠 때 필요해서 test에 선언\n",
    "test = all_df[len(train):]\n",
    "\n",
    "train_set_X = all_df[:len(train)]\n",
    "train_set_y = train_set_X['target']\n",
    "\n",
    "train_set_X = train_set_X[features]\n",
    "\n",
    "# 테스트 셋\n",
    "test_set = all_df[len(train):]\n",
    "test_set = test_set[features]\n",
    "\n",
    "del all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[data['date'] <= '2016-03-27']\n",
    "y_train = x_train['demand']\n",
    "x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "y_val = x_val['demand']\n",
    "test = data[(data['date'] > '2016-04-24')]\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "# define random hyperparammeters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 236,\n",
    "    'learning_rate': 0.1,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 10, \n",
    "    'colsample_bytree': 0.75}\n",
    "\n",
    "train_set = lgb.Dataset(x_train[features], y_train)\n",
    "val_set = lgb.Dataset(x_val[features], y_val)\n",
    "\n",
    "del x_train, y_train\n",
    "\n",
    "model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "val_pred = model.predict(x_val[features])\n",
    "val_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n",
    "print(f'Our val rmse score is {val_score}')\n",
    "y_pred = model.predict(test[features])\n",
    "test['target'] = y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 KFold - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fold = 5\n",
    "# folds = KFold(n_splits=5, shuffle=True)\n",
    "# splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "# y_preds = np.zeros(test.shape[0])\n",
    "# y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "# feature_importances = pd.DataFrame()\n",
    "# feature_importances['feature'] = train_set_X.columns\n",
    "# mean_score = []\n",
    "\n",
    "# for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "#     print('Fold:',fold_n+1)\n",
    "    \n",
    "#     X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "#     y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "#     lgb = LGBMRegressor(\n",
    "#         boosting_type = 'gbdt',\n",
    "#         num_leaves = 2000,\n",
    "#         colsample_bytree = 0.8,\n",
    "#         subsample = 0.8,\n",
    "#         n_estimators = 2500,\n",
    "#         learning_rate = 0.02,\n",
    "#         n_jobs = -1,\n",
    "#         device = 'gpu'\n",
    "#     )\n",
    "#     lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 50, verbose = True)\n",
    "\n",
    "#     feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "#     y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "#     y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "#     val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    \n",
    "#     print(f'val rmse score is {val_score}')\n",
    "    \n",
    "#     mean_score.append(val_score)\n",
    "    \n",
    "#     y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "#     del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# print('mean rmse score over folds is',np.mean(mean_score))\n",
    "# test['target'] = y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 TimeSeriesSplit - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = lgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fold = 10\n",
    "# folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "# splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "# y_preds = np.zeros(test.shape[0])\n",
    "# y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "# feature_importances = pd.DataFrame()\n",
    "# feature_importances['feature'] = train_set_X.columns\n",
    "# mean_score = []\n",
    "\n",
    "# for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "#     print('Fold:',fold_n+1)\n",
    "    \n",
    "#     X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "#     y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "#     lgb = LGBMRegressor(\n",
    "#         num_leaves = 1000,\n",
    "#         colsample_bytree = 0.8,\n",
    "#         subsample = 0.8,\n",
    "#         n_estimators = 2500,\n",
    "#         learning_rate = 0.01,\n",
    "#         n_jobs = -1,\n",
    "#         device = 'cpu'\n",
    "#     )\n",
    "    \n",
    "#     lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 50, verbose = False)\n",
    "\n",
    "#     feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "#     y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "#     y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "#     val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    \n",
    "#     print(f'val rmse score is {val_score}')\n",
    "    \n",
    "#     mean_score.append(val_score)\n",
    "    \n",
    "#     y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "#     del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# print('mean rmse score over folds is',np.mean(mean_score))\n",
    "\n",
    "# test['target'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 feature_importance 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 예측 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "predictions = test[['id', 'date', 'target']]\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'target').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in sub['id'] if 'evaluation' in row] \n",
    "evaluation = sub[sub['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = sub[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "final.to_csv('submissions/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/m5-forecasting-accuracy/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "os.chdir(\"submissions\")\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f submission.csv -m lgb\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 파라미터 및 피처 기록 및 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_record(features, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save model\n",
    "joblib.dump(lgb, 'models/lgb.pkl')\n",
    "# load model\n",
    "# lgb = joblib.load('models/lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
