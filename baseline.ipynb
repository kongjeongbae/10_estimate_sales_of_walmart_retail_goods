{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시계열 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 로드 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 라이브러리 로드 및 메모리 감소 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def write_record(features, params):\n",
    "    record = open(\"record model and features.txt\", 'a')\n",
    "    record.write(\"\\n\")\n",
    "    record.write(str(datetime.datetime.now())+\"\\n\")\n",
    "\n",
    "    check = 0\n",
    "    for _ in features:\n",
    "        check += 1\n",
    "        if check % 5 == 0:\n",
    "            record.write(\"\\n\")\n",
    "        record.write(_+\"  \")\n",
    "    record.write(\"\\n\")\n",
    "    for i  in params.items():\n",
    "        record.write(str(i) + \"\\n\")\n",
    "\n",
    "    record.write('--------------------------------\\n')\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 데이터 불러오기 및 pd.melt를 활용해 데이터 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train = pd.melt(train, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='target')\n",
    "\n",
    "test = pd.read_csv('inputs/sample_submission.csv')\n",
    "test2 = test[30490:]\n",
    "\n",
    "test = test[:30490]\n",
    "test = pd.melt(test, id_vars=['id'], var_name='d', value_name='target')\n",
    "for i in range(1, 29):\n",
    "    test = test.replace({f'F{i}': f'd_{1913+i}'})\n",
    "\n",
    "test[['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id', 'tmp']] = pd.DataFrame(test['id'].str.split('_').tolist())\n",
    "del test['tmp']\n",
    "test['store_id'] = test['state_id'] + '_' + test['store_id']\n",
    "test['dept_id'] = test['cat_id'] + '_' + test['dept_id']\n",
    "test['item_id'] = test['dept_id'] + '_' + test['item_id']\n",
    "\n",
    "test = test[train.columns]\n",
    "\n",
    "calendar = pd.read_csv('inputs/calendar.csv')\n",
    "\n",
    "sell_prices = pd.read_csv('inputs/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아웃라이어를 제거해서 boxplot 더 잘보이게끔\n",
    "train200 = train[train['target'] < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 419)\n",
    "pd.DataFrame(train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "display(\n",
    "    train[train['target'] == 763],\n",
    "    train[train['id'] == 'FOODS_3_090_CA_3_validation'],\n",
    "    sns.distplot(train[train['id'] == 'FOODS_3_090_CA_3_validation']['target'])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "display(\n",
    "    train[train['target'] == 370],\n",
    "    train[train['id'] == 'FOODS_3_318_CA_3_validation'],\n",
    "    sns.distplot(train[train['id'] == 'FOODS_3_318_CA_3_validation']['target'])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train[train['target'] == 763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 시간 오래걸림\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "# sns.distplot(train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby('id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('item_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 dept_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.groupby('dept_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dept_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(train['dept_id'], train200['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('cat_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cat_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby('store_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['store_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store_id 를 기준으로 10가지 모델을 만들어봐도 좋을 듯 싶다.  \n",
    "우선 검증을 해야함. 각각이 많이 다른지부터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 state_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('state_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['state_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('d')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분 date 데이터고, event 및 snap(정부보조금 적용되는 날)에 집중해보자.\n",
    "- event_name_2가 너무 적다. 우선을 빼고 모델 만들어 볼 것임.\n",
    "- 이벤트 유무(is_event) 컬럼 만들 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = calendar.drop(['event_name_2', 'event_type_2'], axis=1)\n",
    "calendar['is_event'] = calendar['event_name_1'].notna().astype('int8')\n",
    "del calendar['wday']\n",
    "del calendar['wm_yr_wk'] # 모든 주차에 인덱스 붙여놓음. 282개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar['day'] = calendar['date'].astype('datetime64').dt.day\n",
    "calendar['week'] = calendar['date'].astype('datetime64').dt.week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(calendar, how='left')\n",
    "test = test.merge(calendar, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day'] = train['date'].astype('datetime64').dt.day\n",
    "test['day'] = test['date'].astype('datetime64').dt.day\n",
    "\n",
    "train['week'] = train['date'].astype('datetime64').dt.week\n",
    "test['week'] = test['date'].astype('datetime64').dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sell_prices['wm_yr_wk'].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.groupby('wm_yr_wk')['sell_price'].mean().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(range(len(sell_prices.groupby('wm_yr_wk')['sell_price'].mean())), list(sell_prices.groupby('wm_yr_wk')['sell_price'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18,
     46
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def write_record(features, params):\n",
    "    record = open(\"record model and features.txt\", 'a')\n",
    "    record.write(\"\\n\")\n",
    "    record.write(str(datetime.datetime.now())+\"\\n\")\n",
    "\n",
    "    check = 0\n",
    "    for _ in features:\n",
    "        check += 1\n",
    "        if check % 5 == 0:\n",
    "            record.write(\"\\n\")\n",
    "        record.write(_+\"  \")\n",
    "    record.write(\"\\n\")\n",
    "    for i  in params.items():\n",
    "        record.write(str(i) + \"\\n\")\n",
    "\n",
    "    record.write('--------------------------------\\n')\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train = pd.melt(train, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='sales')\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('inputs/sample_submission.csv')\n",
    "test = test[:30490]\n",
    "test = pd.melt(test, id_vars=['id'], var_name='d', value_name='sales')\n",
    "for i in range(1, 29):\n",
    "    test = test.replace({f'F{i}': f'd_{1913+i}'})\n",
    "\n",
    "test[['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id', 'tmp']] = pd.DataFrame(test['id'].str.split('_').tolist())\n",
    "del test['tmp']\n",
    "test['store_id'] = test['state_id'] + '_' + test['store_id']\n",
    "test['dept_id'] = test['cat_id'] + '_' + test['dept_id']\n",
    "test['item_id'] = test['dept_id'] + '_' + test['item_id']\n",
    "\n",
    "test = test[train.columns]\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 calendar (date 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('inputs/calendar.csv')\n",
    "\n",
    "calendar = calendar.drop(['event_name_2', 'event_type_2'], axis=1)\n",
    "calendar['is_event'] = calendar['event_name_1'].notna().astype('int8')\n",
    "del calendar['wday']  # weekday랑 똑같은 컬럼.\n",
    "calendar['day'] = calendar['date'].astype('datetime64').dt.day\n",
    "calendar['week'] = calendar['date'].astype('datetime64').dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(calendar, how='left')\n",
    "test = test.merge(calendar, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = pd.read_csv('inputs/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "test = test.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 라벨인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_df = pd.concat([train, test])\n",
    "all_df['revenue'] = all_df['sales'] * all_df['sell_price']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in all_df.columns[all_df.dtypes == 'object']:\n",
    "    if i == 'id' or i == 'date':\n",
    "        continue\n",
    "    all_df[i] = le.fit_transform(list(all_df[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 lag 데이터 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(28, 35):\n",
    "    all_df[f'lag_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(28, 35):\n",
    "    all_df[f'revenue_lag_t{i}'] = all_df.groupby(['id'])['revenue'].transform(lambda x: x.shift(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def make_sales_lag(all_df, lag_day):\n",
    "    for i in range(lag_day, lag_day + 14):\n",
    "        all_df[f'lag_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(i))\n",
    "        if i % 7 == 0:\n",
    "            all_df = reduce_mem_usage(all_df)\n",
    "    \n",
    "    weeks = [7, 28, 56, 112, 168] # 7 30 60 120 180\n",
    "    \n",
    "    for i in weeks:\n",
    "        all_df[f'rolling_max_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).max())\n",
    "        all_df[f'rolling_min_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).min())\n",
    "        all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).mean())\n",
    "        all_df[f'rolling_std_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).std())\n",
    "    \n",
    "    all_df = reduce_mem_usage(all_df)\n",
    "    with open(f'inputs/lag_rolling_{lag_day}.pickle', 'wb') as f:\n",
    "        pickle.dump(all_df, f, protocol=4)\n",
    "\n",
    "# pickle 저장하려고 return 그냥 없앴음\n",
    "#     return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sales_lag(all_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = make_target_lag(all_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 29):\n",
    "#     all_df[f'lag_t{i}'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(i)).fillna(-i)\n",
    "    \n",
    "#     if i % 10 == 7:\n",
    "#         all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# all_df['lag_t28'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28))\n",
    "# all_df['lag_t29'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(29))\n",
    "# all_df['lag_t30'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(30))\n",
    "\n",
    "# # 새롭게 만들 거\n",
    "# all_df['lag_t24'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(24))\n",
    "# all_df['lag_t25'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(25))\n",
    "# all_df['lag_t26'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(26))\n",
    "# all_df['lag_t27'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 이동평균 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 shift를 1한거부터 28한거까지 28개 만들라고 한다.\n",
    "\n",
    "weeks = [7, 28, 56, 84, 112, 168]\n",
    "for i in weeks:\n",
    "    all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(1).rolling(i).min())\n",
    "    all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    all_df[f'rolling_std_t{i}'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "#     all_df = reduce_mem_usage(all_df)\n",
    "print('finish!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weeks = [7, 28, 56, 84, 112, 168] # 7, 30, 60, 90, 120, 180\n",
    "for i in weeks:\n",
    "    all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(i).max())\n",
    "    all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(i).min())\n",
    "    all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(i).mean())\n",
    "    all_df[f'rolling_std_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(i).std())\n",
    "#     all_df = reduce_mem_usage(all_df)\n",
    "print('finish!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[(all_df.week == 17) & (all_df.year == 2015)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연도별 판매량 그래프 분석중\n",
    "\n",
    "- 2016년 4월을 맞춰야 해서, 2015년, 2014년, 2013년을 보고 있는데, 2015년의 그래프가 2014, 2013의 추이에 비해 다른 거 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(all_df[(all_df.week == 17) & (all_df.year == 2013)].groupby('item_id')['target'].mean()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(all_df[(all_df.week == 17) & (all_df.year == 2014)].groupby('item_id')['target'].mean()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(all_df[(all_df.week == 17) & (all_df.year == 2015)].groupby('item_id')['target'].mean()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 17].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 18].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 19].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 20].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 price 통계량 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(28, 35):\n",
    "        all_df[f'price_lag_t{i}'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['rolling_price_std_t14'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(14).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['lag_price_t1'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "\n",
    "all_df['price_change_t1'] = (all_df['lag_price_t1'] - all_df['sell_price']) / (all_df['lag_price_t1'])\n",
    "\n",
    "all_df['rolling_price_max_t365'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "\n",
    "all_df['price_change_t365'] = (all_df['rolling_price_max_t365'] - all_df['sell_price']) / (all_df['rolling_price_max_t365'])\n",
    "\n",
    "all_df['rolling_price_std_t7'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "\n",
    "# 새롭게 만들거\n",
    "# all_df['rolling_price_std_t32'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(32).std())\n",
    "\n",
    "all_df['rolling_price_std_t28'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(28).std())\n",
    "# all_df = reduce_mem_usage(all_df)\n",
    "\n",
    "\n",
    "all_df = all_df.drop(['rolling_price_max_t365', 'lag_price_t1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('inputs/all_df6.pickle', 'wb') as f:\n",
    "    pickle.dump(all_df, f, protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('inputs/all_df.pickle', 'rb') as f:\n",
    "    all_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "# rf.fit(train2, np.log(train['target'] + 1))\n",
    "# result = rf.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMRegressor\n",
    "# # lgb = LGBMRegressor(num_leaves=2000, colsample_bytree=0.6, subsample=0.6, n_estimators=600, learning_rate=0.025, n_jobs=-1, device='gpu', max_bin = 63)\n",
    "# lgb = LGBMRegressor(num_leaves=20, colsample_bytree=0.6, subsample=0.6, n_estimators=60, learning_rate=0.02, n_jobs=-1, device='cpu')\n",
    "\n",
    "# lgb.fit(train, target)\n",
    "# result = lgb.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 KFold - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'weekday', 'event_name_1', 'event_type_1',  \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n",
    "            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30']\n",
    "\n",
    "# 나중에 합칠 때 필요해서 test에 선언\n",
    "test = all_df[len(train):]\n",
    "\n",
    "train_set_X = all_df[:len(train)]\n",
    "train_set_y = train_set_X['target']\n",
    "\n",
    "train_set_X = train_set_X[features]\n",
    "\n",
    "# 테스트 셋\n",
    "test_set = all_df[len(train):]\n",
    "test_set = test_set[features]\n",
    "\n",
    "del all_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 2\n",
    "folds = KFold(n_splits=n_fold, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test.shape[0])\n",
    "y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "eval_results = []\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    \n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        boosting_type = 'gbdt',\n",
    "        num_leaves = 400,\n",
    "        colsample_bytree = 0.8,\n",
    "        subsample = 0.8,\n",
    "        n_estimators = 20,\n",
    "        learning_rate = 0.01,\n",
    "        n_jobs = -1,\n",
    "        device = 'gpu'\n",
    "    )\n",
    "    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 50, verbose = True)\n",
    "    eval_results.append(lgb.evals_result_)\n",
    "    # 피쳐중요도 작성\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "    # validation predict\n",
    "    y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    \n",
    "    print(f'val rmse score is {val_score}')\n",
    "    \n",
    "    mean_score.append(val_score)\n",
    "    \n",
    "    y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['target'] = y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = list(eval_results[0]['valid_0'].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = list(eval_results[1]['valid_0'].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0].values().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_set_X.columns\n",
    "params = lgb.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 KFold - XGB 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)\n",
    "\n",
    "test = all_df[len(train):]\n",
    "\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'weekday', 'event_name_1', 'event_type_1',  \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price']\n",
    "\n",
    "\n",
    "\n",
    "train_set_X = all_df[:len(train)]\n",
    "train_set_y = train_set_X['target']\n",
    "\n",
    "train_set_X = train_set_X[features]\n",
    "\n",
    "# 테스트 셋\n",
    "test_set = all_df[len(train):]\n",
    "test_set = test_set[features]\n",
    "\n",
    "del all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X['sell_price'] = train_set_X['sell_price'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=5, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test.shape[0])\n",
    "# y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "\n",
    "# dtest = xgb.DMatrix(data=test_set)\n",
    "\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.02,subsample=0.8,\n",
    "                max_depth = 12, n_estimators = 4000, tree_method='gpu_hist')\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50)\n",
    "    \n",
    "    # 피쳐중요도 작성\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = xgb_model.feature_importances_\n",
    "    \n",
    "    # validation predict\n",
    "    y_pred_valid = xgb_model.predict(X_valid)\n",
    "    # y_oof[valid_index] = y_pred_valid\n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    print(f'val rmse score is {val_score}')\n",
    "\n",
    "    # test 값 예측\n",
    "    y_preds += xgb_model.predict(test_set) / n_fold\n",
    "    del X_train, X_valid, y_train, y_valid, y_pred_valid, val_score\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['target'] = y_preds\n",
    "\n",
    "features = train_set_X.columns\n",
    "params = xgb_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 TimeSeriesSplit - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fold = 10\n",
    "# folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "# splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "# y_preds = np.zeros(test.shape[0])\n",
    "# y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "# feature_importances = pd.DataFrame()\n",
    "# feature_importances['feature'] = train_set_X.columns\n",
    "# mean_score = []\n",
    "\n",
    "# for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "#     print('Fold:',fold_n+1)\n",
    "    \n",
    "#     X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "#     y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "#     lgb = LGBMRegressor(\n",
    "#         num_leaves = 1000,\n",
    "#         colsample_bytree = 0.8,\n",
    "#         subsample = 0.8,\n",
    "#         n_estimators = 2500,\n",
    "#         learning_rate = 0.01,\n",
    "#         n_jobs = -1,\n",
    "#         device = 'cpu'\n",
    "#     )\n",
    "    \n",
    "#     lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 50, verbose = False)\n",
    "\n",
    "#     feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "#     y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "#     y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "#     val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    \n",
    "#     print(f'val rmse score is {val_score}')\n",
    "    \n",
    "#     mean_score.append(val_score)\n",
    "    \n",
    "#     y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "#     del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# print('mean rmse score over folds is',np.mean(mean_score))\n",
    "\n",
    "# test['target'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 feature_importance 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 예측 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "predictions = test[['id', 'date', 'target']]\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'target').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in sub['id'] if 'evaluation' in row] \n",
    "evaluation = sub[sub['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = sub[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "final.to_csv('submissions/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,29):\n",
    "    final['F'+str(i)] *= 1.04\n",
    "    \n",
    "submission.to_csv('sub.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/m5-forecasting-accuracy/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "os.chdir(\"submissions\")\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f submission.csv -m lgb\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 파라미터, 피처 기록 및 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_record(features, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save model\n",
    "# joblib.dump(lgb, 'models/lgb1.pkl')\n",
    "# load model\n",
    "# lgb = joblib.load('models/lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp1 = pd.read_csv('submissions/submission.csv')\n",
    "tmp2 = pd.read_csv('submissions/fnu050/submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp3 = tmp1[:30490].sort_values('id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp4 = tmp2[:30490]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 29):\n",
    "    tmp3[f'F{i}'] = tmp3[f'F{i}'] * 0.55 + tmp4[f'F{i}'] * 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp5 = pd.concat([tmp3, tmp2[30490:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp5.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
