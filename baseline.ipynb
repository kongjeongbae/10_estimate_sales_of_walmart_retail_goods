{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시계열 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 로드 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 라이브러리 로드 및 메모리 감소 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def write_record(features, params):\n",
    "    record = open(\"record model and features.txt\", 'a')\n",
    "    record.write(\"\\n\")\n",
    "    record.write(str(datetime.datetime.now())+\"\\n\")\n",
    "\n",
    "    check = 0\n",
    "    for _ in features:\n",
    "        check += 1\n",
    "        if check % 5 == 0:\n",
    "            record.write(\"\\n\")\n",
    "        record.write(_+\"  \")\n",
    "    record.write(\"\\n\")\n",
    "    for i  in params.items():\n",
    "        record.write(str(i) + \"\\n\")\n",
    "\n",
    "    record.write('--------------------------------\\n')\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 데이터 불러오기 및 pd.melt를 활용해 데이터 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train = pd.melt(train, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='sales')\n",
    "\n",
    "test = pd.read_csv('inputs/sample_submission.csv')\n",
    "test2 = test[30490:]\n",
    "\n",
    "test = test[:30490]\n",
    "test = pd.melt(test, id_vars=['id'], var_name='d', value_name='sales')\n",
    "for i in range(1, 29):\n",
    "    test = test.replace({f'F{i}': f'd_{1913+i}'})\n",
    "\n",
    "test[['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id', 'tmp']] = pd.DataFrame(test['id'].str.split('_').tolist())\n",
    "del test['tmp']\n",
    "test['store_id'] = test['state_id'] + '_' + test['store_id']\n",
    "test['dept_id'] = test['cat_id'] + '_' + test['dept_id']\n",
    "test['item_id'] = test['dept_id'] + '_' + test['item_id']\n",
    "\n",
    "test = test[train.columns]\n",
    "\n",
    "calendar = pd.read_csv('inputs/calendar.csv')\n",
    "\n",
    "sell_prices = pd.read_csv('inputs/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.read_csv('weights.csv')\n",
    "\n",
    "train['series'] = train['item_id'] + '--' + train['store_id']\n",
    "test['series'] = test['item_id'] + '--' + test['store_id']\n",
    "\n",
    "train = pd.merge(train, weights, how='left', on='series')\n",
    "test = pd.merge(test, weights, how='left', on='series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아웃라이어를 제거해서 boxplot 더 잘보이게끔\n",
    "train200 = train[train['target'] < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 419)\n",
    "pd.DataFrame(train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "display(\n",
    "    train[train['target'] == 763],\n",
    "    train[train['id'] == 'FOODS_3_090_CA_3_validation'],\n",
    "    sns.distplot(train[train['id'] == 'FOODS_3_090_CA_3_validation']['target'])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "display(\n",
    "    train[train['target'] == 370],\n",
    "    train[train['id'] == 'FOODS_3_318_CA_3_validation'],\n",
    "    sns.distplot(train[train['id'] == 'FOODS_3_318_CA_3_validation']['target'])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train[train['target'] == 763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 시간 오래걸림\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "# sns.distplot(train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby('id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('item_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 dept_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.groupby('dept_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dept_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(train['dept_id'], train200['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('cat_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cat_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby('store_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['store_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store_id 를 기준으로 10가지 모델을 만들어봐도 좋을 듯 싶다.  \n",
    "우선 검증을 해야함. 각각이 많이 다른지부터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 state_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('state_id')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['state_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('d')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분 date 데이터고, event 및 snap(정부보조금 적용되는 날)에 집중해보자.\n",
    "- event_name_2가 너무 적다. 우선을 빼고 모델 만들어 볼 것임.\n",
    "- 이벤트 유무(is_event) 컬럼 만들 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = calendar.drop(['event_name_2', 'event_type_2'], axis=1)\n",
    "calendar['is_event'] = calendar['event_name_1'].notna().astype('int8')\n",
    "del calendar['wday']\n",
    "del calendar['wm_yr_wk'] # 모든 주차에 인덱스 붙여놓음. 282개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar['day'] = calendar['date'].astype('datetime64').dt.day\n",
    "calendar['week'] = calendar['date'].astype('datetime64').dt.week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(calendar, how='left')\n",
    "test = test.merge(calendar, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day'] = train['date'].astype('datetime64').dt.day\n",
    "test['day'] = test['date'].astype('datetime64').dt.day\n",
    "\n",
    "train['week'] = train['date'].astype('datetime64').dt.week\n",
    "test['week'] = test['date'].astype('datetime64').dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sell_prices['wm_yr_wk'].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.groupby('wm_yr_wk')['sell_price'].mean().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(range(len(sell_prices.groupby('wm_yr_wk')['sell_price'].mean())), list(sell_prices.groupby('wm_yr_wk')['sell_price'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "\n",
    "        return np.mean(all_scores)\n",
    "\n",
    "train_df = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train_fold_df = train_df.iloc[:, :-28]\n",
    "valid_fold_df = train_df.iloc[:, -28:]\n",
    "valid_preds = valid_fold_df.copy() + np.random.randint(100, size=valid_fold_df.shape)\n",
    "\n",
    "evaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, prices)\n",
    "evaluator.score(valid_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSEForLightGBM(WRMSSEEvaluator):\n",
    "\n",
    "    def feval(self, preds, dtrain):\n",
    "        preds = preds.reshape(self.valid_df[self.valid_target_columns].shape)\n",
    "        score = self.score(preds)\n",
    "        return 'WRMSSE', score, False\n",
    "\n",
    "evaluator = WRMSSEForLightGBM(train_fold_df, valid_fold_df, calendar, prices)\n",
    "\n",
    "model = lgb.train(params, dtrain,\n",
    "                  num_boost_round=10000,\n",
    "                  valid_sets=dvalid,\n",
    "                  feval=evaluator.feval,\n",
    "                  early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18,
     46
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def write_record(features, params):\n",
    "    record = open(\"record model and features.txt\", 'a')\n",
    "    record.write(\"\\n\")\n",
    "    record.write(str(datetime.datetime.now())+\"\\n\")\n",
    "\n",
    "    check = 0\n",
    "    for _ in features:\n",
    "        check += 1\n",
    "        if check % 5 == 0:\n",
    "            record.write(\"\\n\")\n",
    "        record.write(_+\"  \")\n",
    "    record.write(\"\\n\")\n",
    "    for i  in params.items():\n",
    "        record.write(str(i) + \"\\n\")\n",
    "\n",
    "    record.write('--------------------------------\\n')\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('inputs/sales_train_validation.csv')\n",
    "train = pd.melt(train, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='sales')\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('inputs/sample_submission.csv')\n",
    "test = test[:30490]\n",
    "test = pd.melt(test, id_vars=['id'], var_name='d', value_name='sales')\n",
    "for i in range(1, 29):\n",
    "    test = test.replace({f'F{i}': f'd_{1913+i}'})\n",
    "\n",
    "test[['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id', 'tmp']] = pd.DataFrame(test['id'].str.split('_').tolist())\n",
    "del test['tmp']\n",
    "test['store_id'] = test['state_id'] + '_' + test['store_id']\n",
    "test['dept_id'] = test['cat_id'] + '_' + test['dept_id']\n",
    "test['item_id'] = test['dept_id'] + '_' + test['item_id']\n",
    "\n",
    "test = test[train.columns]\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.read_csv('weights.csv')\n",
    "\n",
    "train['series'] = train['item_id'] + '--' + train['store_id']\n",
    "test['series'] = test['item_id'] + '--' + test['store_id']\n",
    "\n",
    "train = pd.merge(train, weights, how='left', on='series')\n",
    "test = pd.merge(test, weights, how='left', on='series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 calendar (date 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('inputs/calendar.csv')\n",
    "\n",
    "calendar = calendar.drop(['event_name_2', 'event_type_2'], axis=1)\n",
    "calendar['is_event'] = calendar['event_name_1'].notna().astype('int8')\n",
    "del calendar['wday']  # weekday랑 똑같은 컬럼.\n",
    "calendar['day'] = calendar['date'].astype('datetime64').dt.day\n",
    "calendar['week'] = calendar['date'].astype('datetime64').dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(calendar, how='left')\n",
    "test = test.merge(calendar, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = pd.read_csv('inputs/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "test = test.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 라벨인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_df = pd.concat([train, test])\n",
    "all_df['revenue'] = all_df['sales'] * all_df['sell_price']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le_classes = []\n",
    "for i in all_df.columns[all_df.dtypes == 'object']:\n",
    "    if i == 'id' or i == 'date':\n",
    "        continue\n",
    "    all_df[i] = le.fit_transform(list(all_df[i]))\n",
    "    le_classes.append(le.classes_.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 lag 데이터 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 35):\n",
    "    all_df[f'lag_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(28, 35):\n",
    "    all_df[f'revenue_lag_t{i}'] = all_df.groupby(['id'])['revenue'].transform(lambda x: x.shift(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def make_sales_lag(all_df, lag_day):\n",
    "    for i in range(lag_day, lag_day + 14):\n",
    "        all_df[f'lag_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(i))\n",
    "        if i % 7 == 0:\n",
    "            all_df = reduce_mem_usage(all_df)\n",
    "    \n",
    "    weeks = [7, 28, 56, 112, 168] # 7 30 60 120 180\n",
    "    \n",
    "    for i in weeks:\n",
    "        all_df[f'rolling_max_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).max())\n",
    "        all_df[f'rolling_min_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).min())\n",
    "        all_df[f'rolling_mean_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).mean())\n",
    "        all_df[f'rolling_std_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag_day).rolling(i).std())\n",
    "    \n",
    "    all_df = reduce_mem_usage(all_df)\n",
    "    with open(f'inputs/lag_rolling_{lag_day}.pickle', 'wb') as f:\n",
    "        pickle.dump(all_df, f, protocol=4)\n",
    "\n",
    "# pickle 저장하려고 return 그냥 없앴음\n",
    "#     return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sales_lag(all_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = make_target_lag(all_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 29):\n",
    "#     all_df[f'lag_t{i}'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(i)).fillna(-i)\n",
    "    \n",
    "#     if i % 10 == 7:\n",
    "#         all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# all_df['lag_t28'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(28))\n",
    "# all_df['lag_t29'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(29))\n",
    "# all_df['lag_t30'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(30))\n",
    "\n",
    "# # 새롭게 만들 거\n",
    "# all_df['lag_t24'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(24))\n",
    "# all_df['lag_t25'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(25))\n",
    "# all_df['lag_t26'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(26))\n",
    "# all_df['lag_t27'] = all_df.groupby(['id'])['target'].transform(lambda x: x.shift(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 이동평균 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weeks = [7, 28, 112, 168] # 7, 30, 60, 90, 120, 180\n",
    "shifts = [7, 28]\n",
    "for j in shifts:\n",
    "    for i in weeks:\n",
    "        all_df[f'rolling_max_s{j}_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(j).rolling(i).max())\n",
    "        all_df[f'rolling_min_s{j}_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(j).rolling(i).min())\n",
    "        all_df[f'rolling_mean_s{j}_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(j).rolling(i).mean())\n",
    "        all_df[f'rolling_std_s{j}_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(j).rolling(i).std())\n",
    "    all_df = reduce_mem_usage(all_df)\n",
    "print('finish!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[(all_df.week == 17) & (all_df.year == 2015)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연도별 판매량 그래프 분석중\n",
    "\n",
    "- 2016년 4월을 맞춰야 해서, 2015년, 2014년, 2013년을 보고 있는데, 2015년의 그래프가 2014, 2013의 추이에 비해 다른 거 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(all_df[(all_df.week == 17) & (all_df.year == 2013)].groupby('item_id')['target'].mean()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(all_df[(all_df.week == 17) & (all_df.year == 2014)].groupby('item_id')['target'].mean()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(all_df[(all_df.week == 17) & (all_df.year == 2015)].groupby('item_id')['target'].mean()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 17].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 18].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 19].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df.week == 20].groupby('year')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 price 통계량 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(28, 35):\n",
    "        all_df[f'price_lag_t{i}'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['rolling_price_std_t14'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(14).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['lag_price_t1'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "\n",
    "all_df['price_change_t1'] = (all_df['lag_price_t1'] - all_df['sell_price']) / (all_df['lag_price_t1'])\n",
    "\n",
    "all_df['rolling_price_max_t365'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "\n",
    "all_df['price_change_t365'] = (all_df['rolling_price_max_t365'] - all_df['sell_price']) / (all_df['rolling_price_max_t365'])\n",
    "\n",
    "all_df['rolling_price_std_t7'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "\n",
    "# 새롭게 만들거\n",
    "# all_df['rolling_price_std_t32'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(32).std())\n",
    "\n",
    "all_df['rolling_price_std_t28'] = all_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(28).std())\n",
    "\n",
    "all_df = all_df.drop(['rolling_price_max_t365', 'lag_price_t1'], axis = 1)\n",
    "# all_df = reduce_mem_usage(all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('inputs/all_df8.pickle', 'wb') as f:\n",
    "    pickle.dump(all_df, f, protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('inputs/all_df8_le_classes.pickle', 'wb') as f:\n",
    "    pickle.dump(le_classes, f, protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('inputs/all_df.pickle', 'rb') as f:\n",
    "    all_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "# rf.fit(train2, np.log(train['target'] + 1))\n",
    "# result = rf.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMRegressor\n",
    "# # lgb = LGBMRegressor(num_leaves=2000, colsample_bytree=0.6, subsample=0.6, n_estimators=600, learning_rate=0.025, n_jobs=-1, device='gpu', max_bin = 63)\n",
    "# lgb = LGBMRegressor(num_leaves=20, colsample_bytree=0.6, subsample=0.6, n_estimators=60, learning_rate=0.02, n_jobs=-1, device='cpu')\n",
    "\n",
    "# lgb.fit(train, target)\n",
    "# result = lgb.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 KFold - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'weekday', 'event_name_1', 'event_type_1',  \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n",
    "            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30']\n",
    "\n",
    "# 나중에 합칠 때 필요해서 test에 선언\n",
    "test = all_df[len(train):]\n",
    "\n",
    "train_set_X = all_df[:len(train)]\n",
    "train_set_y = train_set_X['target']\n",
    "\n",
    "train_set_X = train_set_X[features]\n",
    "\n",
    "# 테스트 셋\n",
    "test_set = all_df[len(train):]\n",
    "test_set = test_set[features]\n",
    "\n",
    "del all_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 2\n",
    "folds = KFold(n_splits=n_fold, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test.shape[0])\n",
    "y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "eval_results = []\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    \n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        boosting_type = 'gbdt',\n",
    "        num_leaves = 400,\n",
    "        colsample_bytree = 0.8,\n",
    "        subsample = 0.8,\n",
    "        n_estimators = 20,\n",
    "        learning_rate = 0.01,\n",
    "        n_jobs = -1,\n",
    "        device = 'gpu'\n",
    "    )\n",
    "    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 50, verbose = True)\n",
    "    eval_results.append(lgb.evals_result_)\n",
    "    # 피쳐중요도 작성\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "    # validation predict\n",
    "    y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    \n",
    "    print(f'val rmse score is {val_score}')\n",
    "    \n",
    "    mean_score.append(val_score)\n",
    "    \n",
    "    y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['target'] = y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = list(eval_results[0]['valid_0'].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = list(eval_results[1]['valid_0'].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0].values().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_set_X.columns\n",
    "params = lgb.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 KFold - XGB 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = reduce_mem_usage(all_df)\n",
    "\n",
    "test = all_df[len(train):]\n",
    "\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'weekday', 'event_name_1', 'event_type_1',  \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price']\n",
    "\n",
    "\n",
    "\n",
    "train_set_X = all_df[:len(train)]\n",
    "train_set_y = train_set_X['target']\n",
    "\n",
    "train_set_X = train_set_X[features]\n",
    "\n",
    "# 테스트 셋\n",
    "test_set = all_df[len(train):]\n",
    "test_set = test_set[features]\n",
    "\n",
    "del all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X['sell_price'] = train_set_X['sell_price'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=5, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test.shape[0])\n",
    "# y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "\n",
    "# dtest = xgb.DMatrix(data=test_set)\n",
    "\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.02,subsample=0.8,\n",
    "                max_depth = 12, n_estimators = 4000, tree_method='gpu_hist')\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50)\n",
    "    \n",
    "    # 피쳐중요도 작성\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = xgb_model.feature_importances_\n",
    "    \n",
    "    # validation predict\n",
    "    y_pred_valid = xgb_model.predict(X_valid)\n",
    "    # y_oof[valid_index] = y_pred_valid\n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    print(f'val rmse score is {val_score}')\n",
    "\n",
    "    # test 값 예측\n",
    "    y_preds += xgb_model.predict(test_set) / n_fold\n",
    "    del X_train, X_valid, y_train, y_valid, y_pred_valid, val_score\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['target'] = y_preds\n",
    "\n",
    "features = train_set_X.columns\n",
    "params = xgb_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 TimeSeriesSplit - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fold = 10\n",
    "# folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "# splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "# y_preds = np.zeros(test.shape[0])\n",
    "# y_oof = np.zeros(train.shape[0])\n",
    "\n",
    "# feature_importances = pd.DataFrame()\n",
    "# feature_importances['feature'] = train_set_X.columns\n",
    "# mean_score = []\n",
    "\n",
    "# for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "#     print('Fold:',fold_n+1)\n",
    "    \n",
    "#     X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "#     y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "#     lgb = LGBMRegressor(\n",
    "#         num_leaves = 1000,\n",
    "#         colsample_bytree = 0.8,\n",
    "#         subsample = 0.8,\n",
    "#         n_estimators = 2500,\n",
    "#         learning_rate = 0.01,\n",
    "#         n_jobs = -1,\n",
    "#         device = 'cpu'\n",
    "#     )\n",
    "    \n",
    "#     lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 50, verbose = False)\n",
    "\n",
    "#     feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "#     y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "#     y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "#     val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    \n",
    "#     print(f'val rmse score is {val_score}')\n",
    "    \n",
    "#     mean_score.append(val_score)\n",
    "    \n",
    "#     y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "#     del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# print('mean rmse score over folds is',np.mean(mean_score))\n",
    "\n",
    "# test['target'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 feature_importance 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 예측 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "predictions = test[['id', 'date', 'target']]\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'target').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in sub['id'] if 'evaluation' in row] \n",
    "evaluation = sub[sub['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = sub[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "final.to_csv('submissions/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,29):\n",
    "    final['F'+str(i)] *= 1.04\n",
    "    \n",
    "submission.to_csv('sub.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/m5-forecasting-accuracy/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "os.chdir(\"submissions\")\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f submission.csv -m lgb\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 파라미터, 피처 기록 및 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_record(features, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save model\n",
    "# joblib.dump(lgb, 'models/lgb1.pkl')\n",
    "# load model\n",
    "# lgb = joblib.load('models/lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp1 = pd.read_csv('submissions/best_submission.csv')\n",
    "tmp2 = pd.read_csv('submissions/submission_v1.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp3 = tmp1.sort_values('id').reset_index(drop=True)\n",
    "tmp4 = tmp2.sort_values('id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 29):\n",
    "    tmp1[f'F{i}'] = tmp1[f'F{i}'] * 0.25+ tmp2[f'F{i}'] * 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp5 = pd.concat([tmp3, tmp3[30490:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
