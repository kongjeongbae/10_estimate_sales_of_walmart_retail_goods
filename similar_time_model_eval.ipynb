{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 시간대를 대상으로 하는 것이 아닌, 예측해야하는 시간대의 target이 비슷한 데이터를 바탕으로\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)\n",
    "- 모델을 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 로드 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from mypackage import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"color:red\">파일명 체크!!!</div>\n",
    "- all_df: 기초작업 수준. \n",
    "- all_df4: lag 데이터 엄청 많이 만들어 놓은 것.\n",
    "- all_df5: 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'month', 'year', 'event_name_1', 'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI', 'is_event', 'day', 'week', 'sell_price', 'revenue', sales_rolling_mean_t7', 'rolling_mean_t7', 'rolling_std_t7', 'sales_rolling_mean_t28', 'rolling_mean_t28', 'rolling_std_t28', 'sales_rolling_mean_t56', 'rolling_mean_t56', 'rolling_std_t56', 'sales_rolling_mean_t112','rolling_mean_t112', 'rolling_std_t112', 'sales_rolling_mean_t168', 'rolling_mean_t168', 'rolling_std_t168', 'lag_t28', 'lag_t29', 'lag_t30', 'lag_t31', 'lag_t32', 'lag_t33', 'lag_t34', 'revenue_lag_t28', 'revenue_lag_t29', 'revenue_lag_t30','revenue_lag_t31', 'revenue_lag_t32', 'revenue_lag_t33', 'revenue_lag_t34'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('data loading')\n",
    "with open('inputs/all_df_eval.pickle', 'rb') as f:\n",
    "    all_df = pickle.load(f)\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(33, 43):\n",
    "    all_df[f'lag_t{i}'] = all_df.groupby(['id'])['sales'].transform(lambda x: x.shift(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['weekofmonth'] = np.ceil(all_df['day'] // 7).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
    "       'sales', 'date', 'wm_yr_wk', 'weekday', 'month', 'year', 'event_name_1',\n",
    "       'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI', 'is_event', 'day',\n",
    "       'week', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30',\n",
    "       'lag_t31', 'lag_t32', 'rolling_mean_t7', 'rolling_std_t7',\n",
    "       'rolling_mean_t28', 'rolling_std_t28', 'rolling_mean_t56',\n",
    "       'rolling_std_t56', 'rolling_mean_t84', 'rolling_std_t84',\n",
    "       'rolling_mean_t112', 'rolling_std_t112', 'rolling_mean_t168',\n",
    "       'rolling_std_t168', 'price_change_t1', 'price_change_t365',\n",
    "       'rolling_price_std_t7', 'rolling_price_std_t28', 'lag_t33', 'lag_t34',\n",
    "       'lag_t35', 'lag_t36', 'lag_t37', 'lag_t38', 'lag_t39', 'lag_t40',\n",
    "       'lag_t41', 'lag_t42', 'weekofmonth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df['rolling_mean_t7'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "# grid_df['rolling_std_t7'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "\n",
    "# 새롭게 만들거\n",
    "# grid_df['rolling_mean_t32'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(32).mean())\n",
    "# grid_df['rolling_std_t32'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(32).std())\n",
    "\n",
    "# grid_df['rolling_mean_t64'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(64).mean())\n",
    "# grid_df['rolling_std_t64'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(64).std())\n",
    "\n",
    "# grid_df['rolling_mean_t96'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(96).mean())\n",
    "# grid_df['rolling_std_t96'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(96).std())\n",
    "\n",
    "# grid_df['rolling_mean_t180'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "# grid_df['rolling_std_t180'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).std())\n",
    "\n",
    "\n",
    "# grid_df['rolling_mean_t30'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "# grid_df['rolling_std_t30'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "\n",
    "\n",
    "# grid_df['rolling_mean_t90'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "# grid_df['rolling_std_t90'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(90).std())\n",
    "\n",
    "# grid_df['rolling_mean_t120'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(120).mean())\n",
    "# grid_df['rolling_std_t120'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(120).std())\n",
    "\n",
    "# grid_df['rolling_mean_t180'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "# grid_df['rolling_std_t180'] = grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 test셋과 비슷한 시간대를 찾아서, 이 데이터만을 바탕으로 모델을 만들것임.  \n",
    "\n",
    "test: 2016-04-25 ~ 2016-05-22  17~20주차 (16주차나 21주차까지 늘려야될지도 생각해보자)\n",
    "\n",
    "---\n",
    "train: 2015-04-27 ~ 2015-05-24  \n",
    "train: 2014-04-28 ~ 2014-05-25  / 2014-04-21 ~ 2014-05-18\n",
    "train: 2013-04-27 ~ 2013-05-24  \n",
    "train: 2012-04-27 ~ 2012-05-24 \n",
    "train: 2011-04-27 ~ 2011-05-24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c00 = all_df['week'] == 9\n",
    "# c01 = all_df['week'] == 10\n",
    "# c02 = all_df['week'] == 11\n",
    "# c03 = all_df['week'] == 12\n",
    "\n",
    "c1 = all_df['week'] == 17\n",
    "c2 = all_df['week'] == 18\n",
    "c3 = all_df['week'] == 19\n",
    "c4 = all_df['week'] == 20\n",
    "\n",
    "c5 = all_df['week'] == 21\n",
    "c6 = all_df['week'] == 22\n",
    "c7 = all_df['week'] == 23\n",
    "c8 = all_df['week'] == 24\n",
    "\n",
    "all_df2 = all_df[c1 | c2 | c3 | c4 | c5 | c6 | c7 | c8]\n",
    "# all_df2 = all_df[c1 | c2 | c3 | c4 | c5 | c6 | c7 | c8 |c00|c01|c02|c03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_gb_sell_price_mean = all_df2.groupby(['id', 'year', 'week'])['sell_price'].mean().rename('item_id_gb_sell_price_mean')\n",
    "all_df2 = pd.merge(all_df2, item_id_gb_sell_price_mean, on=['id', 'year', 'week'], how='left')\n",
    "\n",
    "\n",
    "item_id_gb_sell_price_std = all_df2.groupby(['id', 'year', 'week'])['sell_price'].std().rename('item_id_gb_sell_price_std')\n",
    "all_df2 = pd.merge(all_df2, item_id_gb_sell_price_std, on=['id', 'year', 'week'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_id = LabelEncoder()\n",
    "le_id.fit(all_df2['id'])\n",
    "\n",
    "all_df2['id'] = le_id.transform(all_df2['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training and test data.\n",
    "- 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "- 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "- 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 train set /  test set 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature 선정에 있어서 다른 노트북에서는 all_df = all_df.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1) 이렇게 진행했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df2 = all_df2.rename(columns={'target':'sales'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2011-01-29 ~ 2016-04-24 : d_1 ~ d_1913  \n",
    "2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)  \n",
    "2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = ['sales', 'date', 'wm_yr_wk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X = all_df2[all_df2['date'] <= '2016-05-22']\n",
    "# train_weights = train_set['weight'].copy()\n",
    "# train_set_X = train_set[features]\n",
    "train_set_y = train_set_X['sales']\n",
    "train_set_X = train_set_X.drop(DROP_COLS, axis=1)\n",
    "\n",
    "# 테스트 셋\n",
    "test_set = all_df2[all_df2['date'] > '2016-05-22']\n",
    "test_set = test_set.drop(DROP_COLS, axis=1)\n",
    "\n",
    "\n",
    "var_set_X = all_df2[(all_df2['date'] > '2015-05-22') & (all_df2['date'] <= '2015-06-19')]\n",
    "var_set_y = var_set_X['sales']\n",
    "var_set_X = var_set_X.drop(DROP_COLS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 8\n",
    "folds = KFold(n_splits=n_fold, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test_set.shape[0])\n",
    "y_oof = np.zeros(train_set_X.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "\n",
    "print(train_set_X.columns)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        objective = 'regression',\n",
    "        boosting_type = 'gbdt',\n",
    "        num_leaves = 2048,\n",
    "        colsample_bytree = 0.8,\n",
    "        subsample = 0.8,\n",
    "        n_estimators = 800, ## 중요!!!!\n",
    "        learning_rate = 0.05,\n",
    "        n_jobs = -1,\n",
    "        reg_lambda = 0.1,\n",
    "        device = 'gpu'\n",
    "    )\n",
    "    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 20, verbose = True)\n",
    "\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "    y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "    # validation 측정\n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    print(f'val rmse score is {val_score}')\n",
    "    mean_score.append(val_score)\n",
    "    \n",
    "#     all_pred = lgb.predict(var_set_X, num_iteration=lgb.best_iteration_)\n",
    "#     all_var_score = np.sqrt(metrics.mean_squared_error(all_pred, var_set_y))\n",
    "#     print(f'2015-04-27 부터 2015-05-22까지 데이터로 validation rmse 결과: {all_var_score}')\n",
    "    \n",
    "    \n",
    "    # 예측\n",
    "    y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    \n",
    "params = lgb.get_params()\n",
    "eval_results = lgb.evals_result_['valid_0']['l2']\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['sales'] = y_preds\n",
    "\n",
    "\n",
    "\n",
    "sub = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "predictions = test[['id', 'date', 'sales']]\n",
    "predictions['id'] = list(le_id.inverse_transform(predictions['id']))\n",
    "\n",
    "\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'sales').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in sub['id'] if 'evaluation' in row] \n",
    "evaluation = sub[sub['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = sub[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = all_df2[all_df2['date'] > '2016-05-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sales'] = y_preds\n",
    "\n",
    "sub = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "predictions = test[['id', 'date', 'sales']]\n",
    "predictions['id'] = list(le_id.inverse_transform(predictions['id']))\n",
    "\n",
    "\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'sales').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in sub['id'] if 'validation' in row] \n",
    "evaluation = sub[sub['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = sub[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([evaluation, validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.iloc[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.iloc[30489:30493,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['F1'] *= 3215/3186\n",
    "final['F2'] *= final['F2'].sum() / (final['F2'].sum() + 119)\n",
    "final['F3'] *= 1913/1923\n",
    "final['F4'] /= 1.0073\n",
    "final['F5'] *= (995874/1000000)\n",
    "final['F6'] *= 1.000376\n",
    "import numpy as np\n",
    "np.random.seed(198505)\n",
    "correction = np.random.randint(1000000)/1000000\n",
    "final['F7'] *= correction\n",
    "final['F8'] *= (100-1)/100 + (100-12)/10000\n",
    "final['F11'] = final['F11']\n",
    "for i in range(9,20):\n",
    "    if i!=11:\n",
    "        final['F'+str(i)] *= 1.01 \n",
    "for i in range(20,29):\n",
    "    final['F'+str(i)] *= 1.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.to_csv('submissions/eval_similar_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp1 = pd.read_csv(\"reference/submission_v1_correction2.csv\")\n",
    "tmp1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 29):\n",
    "    final1.iloc[:, i] = final1.iloc[:, i]*0.075 + tmp1.iloc[:, i]*0.925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.to_csv('last_sub.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "os.chdir(\"submissions\")\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f submission.csv -m lgb\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_store(data, store_id):\n",
    "    data = data[data[\"store_id\"] == store_id]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_set(data, drop_cols):\n",
    "\n",
    "    features = data.columns.drop(drop_cols)\n",
    "\n",
    "    train_set = data[data['date'] <= '2016-04-24']\n",
    "    train_set_X = train_set[features]\n",
    "    train_set_y = train_set['target']\n",
    "\n",
    "    # 테스트 셋\n",
    "    test = data[data['date'] > '2016-04-24']\n",
    "    test_set = test[features]\n",
    "\n",
    "    var_set = data[(data['date'] > '2015-04-27') & (data['date'] <= '2015-05-22')]\n",
    "    var_set_X = var_set[features]\n",
    "    var_set_y = var_set['target']\n",
    "    \n",
    "    return train_set_X, train_set_y, test, test_set, var_set_X, var_set_y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['id', 'd', 'target', 'date', 'wm_yr_wk', 'is_event', 'lag_t28', 'lag_t29',\n",
    "       'lag_t30', 'lag_t24', 'lag_t25', 'lag_t26', 'lag_t27']\n",
    "\n",
    "train_set_X, train_set_y, test, test_set, var_set_X, var_set_y = make_train_test_set(all_df2, drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df2[all_df2.store_id == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 KFold - LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def lgb_model(data, drop_cols):\n",
    "    train_set_X, train_set_y, test, test_set, var_set_X, var_set_y = make_train_test_set(data, drop_cols)\n",
    "    n_fold = 5\n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=333)\n",
    "    splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "    y_preds = np.zeros(test_set.shape[0])\n",
    "    y_oof = np.zeros(train_set_X.shape[0])\n",
    "\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = train_set_X.columns\n",
    "    mean_score = []\n",
    "\n",
    "\n",
    "    for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "        print('Fold:',fold_n+1)\n",
    "\n",
    "        X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "        y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "\n",
    "        lgb = LGBMRegressor(\n",
    "            boosting_type = 'gbdt',\n",
    "            num_leaves = 1024,\n",
    "            colsample_bytree = 0.8,\n",
    "            subsample = 0.8,\n",
    "            n_estimators = 300, ## 중요!!!!\n",
    "            learning_rate = 0.1,\n",
    "            n_jobs = -1,\n",
    "            device = 'gpu',\n",
    "            random_state=333\n",
    "        )\n",
    "        lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 20, verbose = True)\n",
    "\n",
    "        feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "\n",
    "        y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "\n",
    "        y_oof[valid_index] = y_pred_valid\n",
    "\n",
    "        # validation 측정\n",
    "        val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "        print(f'val rmse score is {val_score}')\n",
    "        mean_score.append(val_score)\n",
    "\n",
    "        all_pred = lgb.predict(var_set_X, num_iteration=lgb.best_iteration_)\n",
    "        all_var_score = np.sqrt(metrics.mean_squared_error(all_pred, var_set_y))\n",
    "        print(f'2015-04-27 부터 2015-05-22까지 데이터로 validation rmse 결과: {all_var_score}')\n",
    "\n",
    "\n",
    "        # 예측\n",
    "        y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "\n",
    "        # 메모리 정리\n",
    "        del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    features = train_set_X.columns\n",
    "    params = lgb.get_params()\n",
    "    eval_results = lgb.evals_result_['valid_0']['l2']\n",
    "\n",
    "    print('mean rmse score over folds is',np.mean(mean_score))\n",
    "    test['sales'] = y_preds\n",
    "    write_params_features(features, params, eval_results)\n",
    "    save_feature_importance(feature_importances)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test_set.shape[0])\n",
    "y_oof = np.zeros(train_set_X.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "\n",
    "print(train_set_X.columns)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        objective = 'tweedie',\n",
    "        tweedie_variance_power= 1.1,\n",
    "#         objective = 'poisson',\n",
    "        metric = \"rmse\",\n",
    "        boosting_type = 'gbdt',\n",
    "        num_leaves = 128,\n",
    "        colsample_bytree = 0.8,\n",
    "        subsample = 0.8,\n",
    "        n_estimators = 1200, ## 중요!!!!\n",
    "        learning_rate = 0.05,\n",
    "        n_jobs = -1,\n",
    "        reg_lambda = 1,\n",
    "#         device = 'gpu'\n",
    "    )\n",
    "\n",
    "    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds = 20, verbose = True)\n",
    "\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "    y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "    # validation 측정\n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    print(f'val rmse score is {val_score}')\n",
    "    mean_score.append(val_score)\n",
    "    \n",
    "    all_pred = lgb.predict(var_set_X, num_iteration=lgb.best_iteration_)\n",
    "    all_var_score = np.sqrt(metrics.mean_squared_error(all_pred, var_set_y))\n",
    "    print(f'2015-04-27 부터 2015-05-22까지 데이터로 validation rmse 결과: {all_var_score}')\n",
    "    \n",
    "    \n",
    "    # 예측\n",
    "    y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    \n",
    "params = lgb.get_params()\n",
    "eval_results = lgb.evals_result_['valid_0']['l2']\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['sales'] = y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sales'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 베이지안 최적화(Bayesian optimization\n",
    ":  초매개변수(Hyper parameter)를 자동으로 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from functools import partial   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_cv(num_leaves, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda, x_data=None, y_data=None, n_splits=5, output='score'):\n",
    "    score = 0\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    models = []\n",
    "    for train_index, valid_index in kf.split(x_data):\n",
    "        x_train, y_train = x_data.iloc[train_index], y_data[train_index]\n",
    "        x_valid, y_valid = x_data.iloc[valid_index], y_data[valid_index]\n",
    "        \n",
    "        model = LGBMRegressor(\n",
    "            num_leaves = int(num_leaves), \n",
    "            learning_rate = learning_rate, \n",
    "            n_estimators = int(n_estimators), \n",
    "            subsample = np.clip(subsample, 0, 1), \n",
    "            colsample_bytree = np.clip(colsample_bytree, 0, 1), \n",
    "            reg_alpha = reg_alpha, \n",
    "            reg_lambda = reg_lambda,\n",
    "            device = 'gpu'\n",
    "        )\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        models.append(model)\n",
    "        \n",
    "        pred = model.predict(x_valid)\n",
    "        true = y_valid\n",
    "        score += metrics.mean_squared_error(true, pred)/n_splits\n",
    "    \n",
    "    if output == 'score':\n",
    "        return score\n",
    "    if output == 'model':\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 관련없는 변수 고정\n",
    "func_fixed = partial(lgb_cv, x_data=train_set_X.reset_index(drop=True), y_data=train_set_y.reset_index(drop=True), n_splits=5, output='score') \n",
    "# 베이지안 최적화 범위 설정\n",
    "lgbBO = BayesianOptimization(\n",
    "    func_fixed, \n",
    "    {\n",
    "        'num_leaves': (64, 4096),        # num_leaves,       범위(16~1024)\n",
    "        'learning_rate': (0.0001, 0.1),  # learning_rate,    범위(0.0001~0.1)\n",
    "        'n_estimators': (16, 1024),      # n_estimators,     범위(16~1024)\n",
    "        'subsample': (0.4, 1),             # subsample,        범위(0~1)\n",
    "        'colsample_bytree': (0.4, 1),      # colsample_bytree, 범위(0~1)\n",
    "        'reg_alpha': (0, 10),            # reg_alpha,        범위(0~10)\n",
    "        'reg_lambda': (0, 50),           # reg_lambda,       범위(0~50)\n",
    "    }, \n",
    "    random_state=4321                    # 시드 고정\n",
    ")\n",
    "lgbBO.maximize(init_points=5, n_iter=30) # 처음 5회 랜덤 값으로 score 계산 후 30회 최적화\n",
    "\n",
    "# 이 예제에서는 7개 하이퍼 파라미터에 대해 30회 조정을 시도했습니다.\n",
    "# 다양한 하이퍼 파라미터, 더 많은 iteration을 시도하여 최상의 모델을 얻어보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 store 별로 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['id', 'd', 'target', 'date', 'wm_yr_wk', 'is_event', 'lag_t28', 'lag_t29',\n",
    "       'lag_t30', 'lag_t24', 'lag_t25', 'lag_t26', 'lag_t27']\n",
    "\n",
    "test_sets = []\n",
    "for i in range(10):\n",
    "    store_i = split_store(all_df2, i)\n",
    "    test_sets.append(lgb_model(store_i, drop_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    test_sets[0] = pd.concat([test_sets[0], test_sets[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_sets[0].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4.4 keras 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "# model.compile(optimizer = \"rmsprop\", loss = root_mean_squared_error, \n",
    "#               metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential             # Sequential 생성자를 불러옵니다.\n",
    "from keras.layers import Dense, Activation, Dropout      # Dense와 Activation 두 층 인스턴스를 불러옵니다.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal', input_dim=train_set_X[features].shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))                   # 1차원 출력에 'sigmoid' 함수를 적용하는 Dense 층입니다.\n",
    "model.compile(optimizer='rmsprop',                          # 최적화 함수 = 'rmsprop'\n",
    "              loss= 'mse',                   # 손실 함수 = 'binary_crossentropy'\n",
    "              metrics=[root_mean_squared_error])                         # 평가 지표 = 'accuracy'\n",
    "\n",
    "# 학습시키기 \n",
    "model.fit(train_set_X, train_set_y, epochs=1, batch_size=1000, validation_split = 0.2)           # 생성된 데이터를 32개씩의 배치로 나누어 전체를 총 10회 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result_ = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test['target'] = result_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 예측 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "predictions = test[['id', 'date', 'sales']]\n",
    "predictions['id'] = list(le_id.inverse_transform(predictions['id']))\n",
    "\n",
    "\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'sales').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in sub['id'] if 'evaluation' in row] \n",
    "evaluation = sub[sub['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = sub[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "\n",
    "for i in range(1,29):\n",
    "    final['F'+str(i)] *= 1.0315\n",
    "final.to_csv('submissions/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새롭게 구한 값도 보정하니 정확도가 높아짐\n",
    "# 1.04 를 곱하라고 했는데 1.03 곱하니 더 정확도가 높아짐\n",
    "# 이차회귀식으로 계산 결과, 1.0315 곱하는게 가장 좋음.\n",
    "# tmp = pd.read_csv('submissions/sub_dt_lgb.csv')\n",
    "# for i in range(1,29):\n",
    "#     tmp['F'+str(i)] *= 1.0315\n",
    "# tmp.to_csv('submissions/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/m5-forecasting-accuracy/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "os.chdir(\"submissions\")\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f submission.csv -m lgb\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 파라미터 및 피처 기록 및 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_params_features(features, params, eval_results, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_feature_importance(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1등 노트북이랑 비교해서 카테고리형 피처 빠진거있나 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['d', 'sales', 'date', 'wm_yr_wk', 'is_event', 'revenue',\n",
    "             'lag_t1', 'lag_t2', 'lag_t3', 'lag_t4',\n",
    "       'lag_t5', 'lag_t6', 'lag_t7', 'lag_t8', 'lag_t9', 'lag_t10', 'lag_t11',\n",
    "       'lag_t12', 'lag_t13', 'lag_t14', 'lag_t15', 'lag_t16', 'lag_t17',\n",
    "       'lag_t18', 'lag_t19', 'lag_t20', 'lag_t21', 'lag_t22', 'lag_t23',\n",
    "       'lag_t24', 'lag_t25', 'lag_t26', 'lag_t27', 'lag_t32', 'lag_t33', 'lag_t34',\n",
    "       'rolling_max_s7_t7', 'rolling_min_s7_t7', \n",
    "       'rolling_std_s7_t7', 'rolling_max_s7_t28', 'rolling_min_s7_t28',\n",
    "        'rolling_std_s7_t28', 'rolling_max_s7_t112',\n",
    "       'rolling_min_s7_t112', 'rolling_mean_s7_t112', 'rolling_std_s7_t112',\n",
    "       'rolling_max_s7_t168', 'rolling_min_s7_t168', 'rolling_mean_s7_t168',\n",
    "       'rolling_std_s7_t168',  'rolling_max_s28_t7', 'rolling_min_s28_t7',  'rolling_max_s28_t28',\n",
    "       'rolling_min_s28_t28',   'rolling_max_s28_t112', 'rolling_min_s28_t112',  'rolling_std_s28_t112',\n",
    "          'rolling_max_s28_t168', 'rolling_min_s28_t168',    'rolling_std_s28_t168']\n",
    "\n",
    "features = all_df2.columns.drop(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df2.loc[3759341, 'sales'] = 200\n",
    "\n",
    "train_set = all_df2[all_df2['date'] <= '2016-04-24']\n",
    "train_set_X = train_set[features]\n",
    "train_set_y = train_set['sales']\n",
    "\n",
    "# 테스트 셋\n",
    "test = all_df2[all_df2['date'] > '2016-04-24']\n",
    "test_set = test[features]\n",
    "\n",
    "\n",
    "var_set = all_df2[(all_df2['date'] > '2015-04-27') & (all_df2['date'] <= '2015-05-22')]\n",
    "var_set_X = var_set[features]\n",
    "var_set_y = var_set['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True)\n",
    "splits = folds.split(train_set_X, train_set_y)\n",
    "\n",
    "y_preds = np.zeros(test_set.shape[0])\n",
    "y_oof = np.zeros(train_set_X.shape[0])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = train_set_X.columns\n",
    "mean_score = []\n",
    "\n",
    "print(train_set_X.columns)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    print('Fold:',fold_n+1)\n",
    "    \n",
    "    X_train, X_valid = train_set_X.iloc[train_index], train_set_X.iloc[valid_index]\n",
    "    y_train, y_valid = train_set_y.iloc[train_index], train_set_y.iloc[valid_index]\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        objective = 'regression',\n",
    "        boosting_type = 'gbdt',\n",
    "        num_leaves = 2048,\n",
    "        colsample_bytree = 0.8,\n",
    "        subsample = 0.8,\n",
    "        n_estimators = 1200, ## 중요!!!!\n",
    "        learning_rate = 0.05,\n",
    "        n_jobs = -1,\n",
    "        reg_lambda = 1,\n",
    "        device = 'gpu'\n",
    "    )\n",
    "    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds = 20, verbose = True)\n",
    "\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = lgb.feature_importances_\n",
    "    \n",
    "    y_pred_valid = lgb.predict(X_valid, num_iteration=lgb.best_iteration_)\n",
    "    \n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    \n",
    "    # validation 측정\n",
    "    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid))\n",
    "    print(f'val rmse score is {val_score}')\n",
    "    mean_score.append(val_score)\n",
    "    \n",
    "    all_pred = lgb.predict(var_set_X, num_iteration=lgb.best_iteration_)\n",
    "    all_var_score = np.sqrt(metrics.mean_squared_error(all_pred, var_set_y))\n",
    "    print(f'2015-04-27 부터 2015-05-22까지 데이터로 validation rmse 결과: {all_var_score}')\n",
    "    \n",
    "    \n",
    "    # 예측\n",
    "    y_preds += lgb.predict(test_set, num_iteration=lgb.best_iteration_) / n_fold\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    \n",
    "params = lgb.get_params()\n",
    "eval_results = lgb.evals_result_['valid_0']['l2']\n",
    "\n",
    "print('mean rmse score over folds is',np.mean(mean_score))\n",
    "test['sales'] = y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "324px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
